# from langchain_community.vectorstores import Chroma
# from langchain.embeddings import HuggingFaceEmbeddings
# from text_to_doc import get_doc_chunks
# from web_crawler import get_data_from_website
# from dotenv import load_dotenv

# load_dotenv('.env.sh')

# def get_chroma_client():
#     """
#     Returns a chroma vector store instance.

#     Returns:
#         langchain.vectorstores.chroma.Chroma: ChromaDB vector store instance.
#     """
#     embedding_function = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
#     return Chroma(
#         collection_name="website_data",
#         embedding_function=embedding_function,
#         persist_directory="data/chroma")


# # def store_docs(url):
# #     """
# #     Retrieves data from a website, processes it into document chunks, and stores them in a vector store.

# #     Args:
# #         url (str): The URL of the website to retrieve data from.

# #     Returns:
# #         None
# #     """
# #     text, metadata = get_data_from_website(url)
# #     docs = get_doc_chunks(text, metadata)
# #     vector_store = get_chroma_client()
# #     vector_store.add_documents(docs)
# #     vector_store.persist()

# def store_multiple_sites(urls: list):
#     for url in urls:
#         print(f"Scraping from: {url}")
#         try:
#             text, metadata = get_data_from_website(url)
#             docs = get_doc_chunks(text, metadata)
#             vector_store = get_chroma_client()
#             vector_store.add_documents(docs)
#             vector_store.persist()
#         except Exception as e:
#             print(f"Failed to scrape {url}: {e}")


# from langchain.chat_models import ChatOpenAI
# from prompt import get_prompt
# from langchain.chains import ConversationalRetrievalChain

# def make_chain():
#     """
#     Creates a chain of langchain components.

#     Returns:
#         langchain.chains.ConversationalRetrievalChain: ConversationalRetrievalChain instance.
#     """
#     model = ChatOpenAI(
#         model_name="llama3-70b-8192",  # or another Groq-supported model
#         temperature=0.0,
#         verbose=True,
#     )
#     vector_store = get_chroma_client()
#     prompt = get_prompt()

#     retriever = vector_store.as_retriever(search_type="mmr", verbose=True)

#     chain = ConversationalRetrievalChain.from_llm(
#         model,
#         retriever=retriever,
#         return_source_documents=True,
#         combine_docs_chain_kwargs=dict(prompt=prompt),
#         verbose=True,
#         rephrase_question=False,
#     )
#     return chain


# def get_response(question, organization_name, organization_info, contact_info):
#     """
#     Generates a response based on the input question.

#     Args:
#         question (str): The input question to generate a response for.
#         organization_name (str): The name of the organization.
#         organization_info (str): Information about the organization.
#         contact_info (str): Contact information for the organization.

#     Returns:
#         str: The response generated by the chain model.
#     """
#     chat_history = ""
#     chain = make_chain()
#     response = chain({"question": question, "chat_history": chat_history,
#                       "organization_name": organization_name, "contact_info": contact_info,
#                       "organization_info": organization_info})
#     return response['answer']
#-----------------
from langchain_community.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from text_to_doc import get_doc_chunks
from web_crawler import get_data_from_website
from dotenv import load_dotenv

load_dotenv('.env.sh')

def get_chroma_client():
    """
    Returns a chroma vector store instance.
    """
    embedding_function = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    return Chroma(
        collection_name="website_data",
        embedding_function=embedding_function,
        persist_directory="data/chroma")


def store_multiple_sites(urls: list, user_query: str):
    """
    Retrieves data from a list of websites, passing the user_query for context.
    """
    for url in urls:
        print(f"Scraping from: {url}")
        try:
            # Pass the user_query to the scraping function
            text, metadata = get_data_from_website(url, user_query)
            if "No products found" in text or "No relevant products" in text:
                print(f"Warning: {text} from {url}")
                continue
            docs = get_doc_chunks(text, metadata)
            vector_store = get_chroma_client()
            vector_store.add_documents(docs)
            vector_store.persist()
        except Exception as e:
            print(f"Failed to process {url}: {e}")


from langchain.chat_models import ChatOpenAI
from prompt import get_prompt
from langchain.chains import ConversationalRetrievalChain

def make_chain():
    """
    Creates a chain of langchain components.
    """
    model = ChatOpenAI(
        model_name="llama3-70b-8192",
        temperature=0.0,
        verbose=True,
    )
    vector_store = get_chroma_client()
    prompt = get_prompt()

    retriever = vector_store.as_retriever(search_type="mmr", verbose=True)

    chain = ConversationalRetrievalChain.from_llm(
        model,
        retriever=retriever,
        return_source_documents=True,
        combine_docs_chain_kwargs=dict(prompt=prompt),
        verbose=True,
        rephrase_question=False,
    )
    return chain


def get_response(question):
    """
    Generates a response based on the input question.
    """
    chat_history = ""
    chain = make_chain()
    response = chain({"question": question, "chat_history": chat_history})
    return response['answer']